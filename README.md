
# â„ï¸ ETL Autorizaciones SRI

Pipeline ETL diseÃ±ado para automatizar la extracciÃ³n, transformaciÃ³n y carga de datos de autorizaciones emitidas por el Servicio de Rentas Internas (SRI) del Ecuador. El sistema estÃ¡ implementado en **Google Cloud Platform (GCP)** utilizando **Apache Airflow (local en Docker)**, **BigQuery** y **Cloud Storage**, permitiendo una integraciÃ³n reproducible, trazable y escalable para el anÃ¡lisis de datos tributarios mediante un modelo **copo de nieve**.

---

## ğŸ¯ Objetivo General

Automatizar la ingesta y transformaciÃ³n de los registros de autorizaciones emitidas por el SRI a contribuyentes, organizados bajo un **modelo dimensional copo de nieve**, que permita consultas analÃ­ticas eficientes y generaciÃ³n de reportes OLAP.

---

## ğŸ§° Herramientas y TecnologÃ­as Utilizadas

| TecnologÃ­a | DescripciÃ³n |
|-----------|-------------|
| **Google Cloud Platform (GCP)** | Infraestructura en la nube |
| **Google Cloud Storage (GCS)** | Almacenamiento de archivos fuente `.csv` |
| **Google BigQuery** | AlmacÃ©n de datos (Data Warehouse) |
| **Apache Airflow (Docker)** | Orquestador ETL desplegado de forma local |
| **Python 3** | Lenguaje principal del pipeline |
| **Pandas** | TransformaciÃ³n y limpieza de datos |
| **google-cloud-storage** | Cliente Python para GCS |
| **google-cloud-bigquery** | Cliente Python para BigQuery |
| **PyArrow** | Backend de exportaciÃ³n para pandas |
| **Docker** | Contenedor de ejecuciÃ³n para Airflow |

---

## ğŸ§± Arquitectura del Proyecto

```
+--------------------------+       +---------------------------+       +---------------------------+
| ğŸ“¥ Archivos CSV (SRI)    | --->  | â˜ï¸ Google Cloud Storage    | --->  | ğŸ³ Apache Airflow (Docker) |
| (2022, 2023, 2024)       |       | (3 archivos cargados)     |       | (TransformaciÃ³n ETL)      |
+--------------------------+       +---------------------------+       +---------------------------+
                                                                              |
                                                                              v
                                                                    +-------------------------+
                                                                    | ğŸ“Š BigQuery DW           |
                                                                    | Modelo Copo de Nieve     |
                                                                    +-------------------------+
```

---

## ğŸ“¦ Componentes Principales

| Componente | DescripciÃ³n |
|------------|-------------|
| **Cloud Storage** | Contiene los archivos fuente `.csv` |
| **Airflow (Docker)** | Ejecuta el DAG localmente con tareas programadas |
| **BigQuery** | Almacena las tablas del modelo copo de nieve |
| **Python (pandas)** | Realiza la transformaciÃ³n, joins y validaciÃ³n |

---

## â„ï¸ Modelo Dimensional - Esquema Copo de Nieve

| Tabla | DescripciÃ³n |
|-------|-------------|
| **hechos_autorizaciones** | Tabla de hechos principal con autorizaciones emitidas |
| **dim_contribuyente** | InformaciÃ³n Ãºnica por RUC (nombre, tipo y ubicaciÃ³n) |
| **dim_tiempo** | DimensiÃ³n temporal por dÃ­a |
| **dim_geografia** | SubdimensiÃ³n: Provincias del Ecuador |
| **dim_tipo_ruc** | SubdimensiÃ³n: Tipos de contribuyentes (Natural, Privada, PÃºblica) |

---

## ğŸ› ï¸ Requisitos del Entorno

### 1. Infraestructura

- Bucket GCS creado: `etl-autorizaciones-datos`
- Dataset BigQuery: `dw_autorizaciones`
- Airflow ejecutÃ¡ndose localmente en Docker

### 2. Permisos y Credenciales

Cuenta de servicio con los siguientes roles:

- `roles/bigquery.dataEditor`
- `roles/bigquery.jobUser`
- `roles/storage.objectViewer`
- `roles/storage.admin`

> Guardar las credenciales `.json` en la carpeta `/include` de Airflow local o montarlas como volumen.

### 3. LibrerÃ­as Python necesarias

```bash
pip install pandas pyarrow
pip install google-cloud-bigquery google-cloud-storage google-auth
```

---

## ğŸ§¬ Estructura del DAG en Airflow

```
etl_autorizaciones_sri_completo
â”œâ”€â”€ inicio
â”œâ”€â”€ cargar_dim_tiempo
â”œâ”€â”€ cargar_dim_geografia
â”œâ”€â”€ cargar_dim_tipo_ruc
â”œâ”€â”€ cargar_dim_contribuyente
â”œâ”€â”€ cargar_hechos_autorizaciones
â””â”€â”€ fin
```

### FunciÃ³n de cada tarea

| Tarea | DescripciÃ³n |
|-------|-------------|
| `inicio` | Inicio del flujo ETL |
| `cargar_dim_tiempo` | Genera fechas del 2022 al 2024 |
| `cargar_dim_geografia` | Carga provincias de Ecuador |
| `cargar_dim_tipo_ruc` | Crea tipos de contribuyentes |
| `cargar_dim_contribuyente` | Une RUC con provincia y tipo |
| `cargar_hechos_autorizaciones` | Realiza join y carga de hechos |
| `fin` | Finaliza el flujo |

---

## ğŸ“‚ UbicaciÃ³n del DAG y Credenciales

- DAG `.py` debe ser montado en el contenedor: `./dags/etl_autorizaciones_sri_completo.py`
- Credenciales GCP: `./include/key-etl-sri.json`

---

## âœ… Resultados Esperados

- Tablas `dim_*` y `hechos_autorizaciones` en BigQuery pobladas correctamente.
- Claves forÃ¡neas resueltas entre contribuyentes, tipos de RUC, geografÃ­a y tiempo.
- Airflow muestra DAG en estado exitoso (âœ… verde).
- Posibilidad de consultas OLAP inmediatas en BigQuery.

---

## ğŸ“· Evidencias

- EjecuciÃ³n exitosa del DAG en Airflow.
- Conteo de registros en BigQuery:
```sql
SELECT COUNT(*) FROM `dw_autorizaciones.hechos_autorizaciones`;
```
- ValidaciÃ³n de claves forÃ¡neas con joins:
```sql
SELECT COUNT(*) 
FROM `dw_autorizaciones.hechos_autorizaciones` h
JOIN `dw_autorizaciones.dim_contribuyente` c ON h.id_contribuyente = c.id_contribuyente;
```

---

## ğŸ“š Recursos Adicionales

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Google BigQuery Python Client](https://cloud.google.com/bigquery/docs/reference/libraries)
- [SRI Ecuador](https://www.sri.gob.ec/)

---

## ğŸ‘¨â€ğŸ’» Autor

**Danilo Guillermo Camacho LeÃ³n**  
Estudiante de IngenierÃ­a de Software  
Escuela Superior PolitÃ©cnica de Chimborazo (ESPOCH)  
Proyecto acadÃ©mico: ETL Autorizaciones SRI

